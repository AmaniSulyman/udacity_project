{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Reporting: wragle_report\n",
    "\n",
    "\n",
    "\n",
    "## Introduction:\n",
    "The Wrangle and Analyze Data project produces a wrangle report as a record of the data wrangling efforts. The dataset used in t is the tweet archive of Twitter user@dog_rates, called WeRateDogs. WeRateDogs is a Twitter account that rates people’s dogs with humorous comment about the dog.\n",
    "\n",
    "##### The three processes of data wrangling are documented in the wrangle report.\n",
    ">▪ Gathering Data\n",
    "\n",
    ">▪ Assessing Data\n",
    "\n",
    ">▪ Cleaning Data\n",
    "\n",
    "also Storing, analyzing and visualizing the wrangled data\n",
    "\n",
    "## Gathering Data: \n",
    "In this project, the data was gathered from several sources and different formats. \n",
    "#### The WeRateDogs Twitter archive;\n",
    "The twitter_archive_enhanced.csv file was provided by Udacity and can be directly downloaded. the archive contains some tweet data like tweet ID, timestamp, text, etc.\n",
    "\n",
    "#### The tweet image predictions: \n",
    "The file is hosted on Udacity’s servers. it has been downloaded this file programmatically by using the Requests library in Python. \n",
    "\n",
    "#### Data via the Twitter API:\n",
    "file contains data that should be obtained from the Twitter handle by using tweepy library but I used the file provided by udacity since I couldn't manage a Twitter API\n",
    "\n",
    "##### The 3 obtained data frames are:-\n",
    "\n",
    ">twitter_archive_df: contains data read from provided csv\n",
    "\n",
    ">image_predictions_df: contains data read (by using requests) from tsv file hosted on the server\n",
    "\n",
    ">tweet_json: contains data that should be obtained from the Twitter handle by using tweepy library but I used the file provided by udacity since I couldn't manage a Twitter API\n",
    "\n",
    "\n",
    "## Assessing data\n",
    "After gathering data from its sources, the data is assessed in two ways which are visually and programmatically to identify any data quality and tidiness issues. As quality relates to content while tidiness relates to the data structure.\n",
    "\n",
    "### Quality issues\n",
    "####  Twitter Archived Enhanced Data\n",
    "1. The name column has many invalid values like , a, an, the.\n",
    "\n",
    "2. In some columns null objects are addressed as \"None\" and some \"NaN\"\n",
    "\n",
    "3. some records have more than one dog stage\n",
    "\n",
    "4. delete retweets and replys\n",
    "\n",
    "5. The Two columens \"retweeted_status_timestamp\" and \"timestamp\" should be string instead of object & The columns \"n_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id\" should be integers/strings instead of float.\n",
    "\n",
    "6. The ratings are not extracted correctly fix rating numerator decimals.\n",
    "\n",
    "7. The tweet_id column should be named same in all the DataFrames and it's datatype should be same in all the tables\n",
    "\n",
    "####  Image Predictions data\n",
    ">8.The column \"tweet_id\" is int while it should be string.\n",
    "\n",
    "### Tidiness issues\n",
    "####  Twitter Archived Enhanced Data\n",
    ">1.The three columns \"doggo\", \"floofer\", \"pupper\", and \"puppo\" needs to be joined to one column, then need to be dropped from the data set.\n",
    "\n",
    ">2.The \"created_at\" column can be split to \"day\", \"month\", and \"year\" columns to obtain more better insights.\n",
    "\n",
    "## Cleaning data\n",
    "\n",
    "To the best of my ability, I was able to overcome the identified issues using my understanding of Python and online resources like Google, Stack Overflow, etc. \n",
    "\n",
    "While assessing, I cleaned each of the issues stated. Even though the entire dataset has a lot of issues, fixing them all would take a lot of time. I, therefore, concentrated only on those relevant to the analysis. The three steps used in the data cleaning process are (defined, code, and test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
